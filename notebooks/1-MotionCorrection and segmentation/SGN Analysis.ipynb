{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Segmentation and motion correction of in-vivo recordings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before running this notebook, generate an ROI called `correctionReference.npy` using [Robopy](https://github.com/fedeceri85/robopy2). This ROI should be placed in the same folder as the recording and contain a trace where the \"jumps\" are visible as decreases (ideally) or increases in fluorescence.\n",
    "\n",
    "This notebook contains the preprocessing steps and segmentation routines for in vivo recordings. Some utilities used in this notebook are:\n",
    "\n",
    "- `thorlabsFile` from `movieTools`: This class contains the actual recording data, functions to smooth and filter it, and display it in a Napari viewer. The `thorlabsFile` object is passed to other routines to extract data. When loading files, they are automatically smoothed with a 3D Gaussian kernel (2x2x2).\n",
    "- `jumpFramesFinder` from `visualisationTools`: This displays an ipywidget interface with tools to remove out-of-focus frames. This interface displays the `correctionReference` generated using Robopy (first step of the analysis).\n",
    "- `jupyterpy` from `movieTools`: This interface connects to the currently displayed `thorlabsFile` and contains buttons for the segmentation of hair cells, calcium waves, and fibers.\n",
    "- `maskMatching` from `movieTools`: This is an ipywidgets/Napari interface that combines repetitive recordings of the same field of view to generate a `matchingMask`, i.e., a label image where hair cell ROIs are color-coded with the same color.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pylab\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import os\n",
    "from scipy.signal import savgol_filter\n",
    "from scipy.signal import find_peaks\n",
    "from scipy.signal import argrelextrema,argrelmin,argrelmax\n",
    "import sys\n",
    "sys.path.append('../../src')\n",
    "\n",
    "import seaborn as sns\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact, Dropdown\n",
    "import visualisationTools as vu\n",
    "import traceUtilities as tu\n",
    "import ast \n",
    "from movieTools import thorlabsFile\n",
    "\n",
    "parameterFolder = '../../parameters/'\n",
    "fileHeader = 'NeuroD_SGN' #SNAP25_SGN\n",
    "\n",
    "inputFilename = os.path.join('../../',fileHeader)+'.xlsx'\n",
    "corrFilename = os.path.join(parameterFolder,fileHeader)+ '_with_corr_params.csv'\n",
    "jumpFrameFilename = os.path.join(parameterFolder,fileHeader+'jump_frames.csv')\n",
    "jumpFrameMaxFilename = os.path.join(parameterFolder,fileHeader+'jumpMax_frames.csv')\n",
    "correctionReferenceTraceFile = os.path.join(parameterFolder,fileHeader+'correctionReference.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we load all the excel files above as pandas dataframes, or create an empty dataframe if they don't exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "alldata = pd.read_excel(inputFilename)\n",
    "alldata = alldata[alldata['discard']!=1]\n",
    "\n",
    "try:\n",
    "    corr_param = pd.read_csv(corrFilename)\n",
    "except:\n",
    "    print('No jump correction parameter file')\n",
    "\n",
    "try:\n",
    "    allminima = pd.read_csv(jumpFrameFilename)\n",
    "except FileNotFoundError:\n",
    "    allminima = pd.DataFrame()\n",
    "\n",
    "try:\n",
    "    allmaxima = pd.read_csv(jumpFrameMaxFilename)\n",
    "except FileNotFoundError:\n",
    "    allmaxima = pd.DataFrame()\n",
    "\n",
    "\n",
    "try:\n",
    "    correctionReferenceTrace = pd.read_csv(correctionReferenceTraceFile)\n",
    "except FileNotFoundError:\n",
    "    correctionReferenceTrace = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we go through the list of recordings and compare it to the list of parameters. We copy over the already calculated parameters for all the files that have already been processed, and use default (blank) values for all the new files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for el in alldata['Folder'].unique():\n",
    "    try:\n",
    "        alldata.loc[alldata['Folder']==el,'Window left'] = corr_param.loc[corr_param['Folder']==el,'Window left'].values[0]\n",
    "        alldata.loc[alldata['Folder']==el,'Window right'] = corr_param.loc[corr_param['Folder']==el,'Window right'].values[0]\n",
    "        alldata.loc[alldata['Folder']==el,'Minima order'] = corr_param.loc[corr_param['Folder']==el,'Minima order'].values[0]\n",
    "\n",
    "\n",
    "\n",
    "    except (IndexError, NameError, KeyError):\n",
    "        print('Could not find correction parameters for '+ el+' Setting to default.')\n",
    "        alldata.loc[alldata['Folder']==el,'Window left'] = 0\n",
    "        alldata.loc[alldata['Folder']==el,'Window right'] = 0\n",
    "        alldata.loc[alldata['Folder']==el,'Minima order'] = 50 \n",
    "\n",
    "\n",
    "\n",
    "    try:\n",
    "        alldata.loc[alldata['Folder']==el,'Window Max left'] = corr_param.loc[corr_param['Folder']==el,'Window Max left'].values[0]\n",
    "        alldata.loc[alldata['Folder']==el,'Window Max right'] = corr_param.loc[corr_param['Folder']==el,'Window Max right'].values[0]\n",
    "        alldata.loc[alldata['Folder']==el,'Maxima order'] = corr_param.loc[corr_param['Folder']==el,'Maxima order'].values[0]\n",
    "    except (IndexError, NameError, KeyError):\n",
    "        alldata.loc[alldata['Folder']==el,'Window Max left'] = 0\n",
    "        alldata.loc[alldata['Folder']==el,'Window Max right'] = 0\n",
    "        alldata.loc[alldata['Folder']==el,'Maxima order'] = 50\n",
    "\n",
    "alldata.loc[alldata['Window left'].isna(),'Window left'] = 0\n",
    "alldata.loc[alldata['Window right'].isna(),'Window right'] = 0\n",
    "alldata.loc[alldata['Minima order'].isna(),'Minima order'] = 50\n",
    "\n",
    "alldata.loc[alldata['Folder'].isna(),'Window Max left'] = 0\n",
    "alldata.loc[alldata['Folder'].isna(),'Window Max right'] = 0\n",
    "alldata.loc[alldata['Folder'].isna(),'Maxima order'] = 50\n",
    "\n",
    "alldata['Window left'] = alldata['Window left'].astype(int)\n",
    "alldata['Window right'] = alldata['Window right'].astype(int)\n",
    "alldata['Minima order'] = alldata['Minima order'].astype(int)\n",
    "\n",
    "alldata['Window Max left'] = alldata['Window Max left'].astype(int)\n",
    "alldata['Window Max right'] = alldata['Window Max right'].astype(int)\n",
    "alldata['Maxima order'] = alldata['Maxima order'].astype(int)\n",
    "\n",
    "\n",
    "alldata.loc[alldata['Minima order']==1,'Minima order'] = 50\n",
    "alldata.loc[alldata['Maxima order']==1,'Maxima order'] = 50\n",
    "\n",
    "try:\n",
    "    for el in alldata['Folder'].unique():\n",
    "        alldata.loc[alldata['Folder']==el,'ExtraCorrectionIntervals'] = corr_param.loc[corr_param['Folder']==el,'ExtraCorrectionIntervals'].values[0]\n",
    "except:\n",
    "    pass\n",
    "\n",
    "for j,el in alldata.iterrows():\n",
    "    try:\n",
    "        alldata.at[j,'ExtraCorrectionIntervals'] = ast.literal_eval(alldata.at[j,'ExtraCorrectionIntervals'] )\n",
    "    except (ValueError,KeyError):\n",
    "         alldata.at[j,'ExtraCorrectionIntervals'] = np.nan\n",
    "alldata['ExtraCorrectionIntervals'] =  alldata['ExtraCorrectionIntervals'].astype('object')\n",
    "\n",
    "\n",
    "try:\n",
    "    for el in alldata['Folder'].unique():\n",
    "        alldata.loc[alldata['Folder']==el,'TemplateIntervals'] = corr_param.loc[corr_param['Folder']==el,'TemplateIntervals'].values[0]\n",
    "except:\n",
    "    pass\n",
    "\n",
    "for j,el in alldata.iterrows():\n",
    "    try:\n",
    "        alldata.at[j,'TemplateIntervals'] = ast.literal_eval(alldata.at[j,'TemplateIntervals'] )\n",
    "    except (ValueError,KeyError):\n",
    "         alldata.at[j,'TemplateIntervals'] = np.nan\n",
    "\n",
    "alldata['TemplateIntervals'] =  alldata['TemplateIntervals'].astype('object')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master = alldata.copy().reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1- Jump Correction. \n",
    "Remember to run the block after this every now and then to save the results in case this notebook crashes\n",
    "`jumpFramesFinder` is a comprehensive tool that combines interactive widgets, data processing, and visualization to assist users in identifying and correcting frame jumps in their datasets.\n",
    "The jumpFramesFinder function is designed to facilitate the identification and correction of frame jumps in a in vivo movie. This function utilizes a variety of widgets from the `ipywidgets` library to create an interactive user interface  that allows users to adjust parameters and visualize the effects of these adjustments in real-time.\n",
    "\n",
    "The user can utilise various sliders to control various parameters such as the trace number, window sizes for minima and maxima detection, and the order of minima and maxima. These sliders allow users to fine-tune the detection of frame jumps by adjusting the sensitivity and range of the detection algorithm.\n",
    "\n",
    "Users can perform specific actions, such as loading original or corrected movie, saving processed files, and creating templates for further analysis.\n",
    "\n",
    "The function also sets up a plot using `plotly.graph_objects` to visualize the original and corrected traces, as well as the detected jumps. This visualization helps users to see the impact of their adjustments and make decisions about the parameters they set.\n",
    "\n",
    "The corrected movie is visualised through the `thorlabsFile` object in a napari window.\n",
    "\n",
    "After this step, launch the `batchModtionCorrect.ipynb` notebook to motion correct the \"jumpCorrected.tif\" files, which will be saved as `jumpCorrected-mc.tif` files. After that, come back to this notebook, and run this block to load the motion corrected files, to be used for segmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tb = thorlabsFile()\n",
    "vu.jumpFramesFinder(master,allminima,allmaxima,correctionReferenceTrace,tb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save parameters block. Uncomment it and run it while doing the previous step to prevent data loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#master.to_csv(corrFilename)\n",
    "#allminima.to_csv(jumpFrameFilename)\n",
    "#allmaxima.to_csv(jumpFrameMaxFilename)\n",
    "#correctionReferenceTrace.to_csv(correctionReferenceTraceFile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The next block displays the jupyterpy interface. This connects to the opened image in napari (the current thorlabsFile object, it should be loaded using the jumpFramesFinder above) and provides buttons with specific analysis steps for each type of experiment.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The jupyterPy function is designed to create an interactive user interface (UI) for analyzing image data within a Jupyter notebook. This function utilizes the ipywidgets library to create various interactive widgets, such as buttons, sliders, and text inputs, which are then organized into a tabbed layout for different analysis tasks.\n",
    "The istance of jupyterPy connects to the movie displayed in the existing napari windows (passed through the thorlabsFile object.)\n",
    "\n",
    "The software allows to perform specific segmentation analysis for IHCs, Calcium waves, and afferent terminals. \n",
    "For IHCs, the software uses cellpose to automatically identify the cell bodies. The segmentation is shown as a `label` layer (named 'Masks') in Napari. Users can modify the existing ROIs using Napari tools (e.g, splitting ROIs). The function also allows to plot and save the fluorescence profile of the individual ROIs. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from movieTools import jupyterPy\n",
    "JP = jupyterPy(tb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check that the masks are sequentials and that there are no sgorbietti in the ihcs\n",
    "# These are not corrected at the moment, but keep in mind that a few recordings have small or non sequential rois"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.io import imread\n",
    "from skimage import measure\n",
    "allareas = []\n",
    "for i,el in enumerate(master['Folder'].unique()):\n",
    "   # print(el)\n",
    "    try:\n",
    "        mask = imread('D'+el[1:]+'\\\\processedMovies\\\\Masks.tif')\n",
    "        \n",
    "        props = measure.regionprops(mask)\n",
    "        labels = [p.label for p in props]\n",
    "        areas = [p.area for p in props]\n",
    "        centroid_x = [p.centroid[0] for p in props]\n",
    "        centroid_y = [p.centroid[1] for p in props]\n",
    "        allareas.extend(areas)\n",
    "        #Check small rois\n",
    "        for area in areas:\n",
    "            if area<100:\n",
    "                print('Small ROI:{} {}'.format(i,el))\n",
    "        #Check if labels are sequential\n",
    "        if sum(diff(labels)<=0):\n",
    "            print('Labels not sequential:{} {}'.format(i,el))\n",
    "        \n",
    "        #Check missing rois\n",
    "        if max(labels)!=len(labels):\n",
    "            print('Missing rois: {} {}'.format(i,el))\n",
    "            \n",
    "        #Check non sequential ROIs\n",
    "        if sum(diff(centroid_y)<=0):\n",
    "            print('ROI positions not sequential:{} {}'.format(i,el))\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tb.app.add_image(avg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2- Match ROIs from different recordings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Change drive \n",
    "localDrive = 'E'\n",
    "master['Folder'] = localDrive + master['Folder'].str[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from movieTools import maskMatching,extractImagesMaskMatching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "maskMatching(master,onlyIHCs=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3- Calculate the local pixel correlation\n",
    "This block calculates how much the pixels belonging to the same ROI are correlated. This helps with peaks identification, as peaks resulting from artifacts (e.g., cross-talk from a neighboring cell) will have a low correlation and can be rejected on this basis\n",
    "The correlation traces are saved in the same folder as the traces (processedMovies folder inside the experiment folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for j,el in master.iterrows():\n",
    "    \n",
    "    if not os.path.exists(os.path.join(el['Folder'],'processedMovies','traces_localCorr.csv')):\n",
    "        print(el['Folder'])\n",
    "        print(j)\n",
    "        try:\n",
    "            df = tu.calculatePixelRollingCorr(el['Folder'],int(el['fps']/2),8,addNoise=True,maskfilename='SGN ROIs.tif')\n",
    "            df.to_csv(os.path.join(el['Folder'],'processedMovies','traces_localCorr.csv'))\n",
    "        except FileNotFoundError:\n",
    "            print('fnf')\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# N numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master.groupby(['Age','Mouse ID']).count()['Folder']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "fa8522bbf777dbdc3b75aca28b76ffd20090ed2cb5a7bbfec3f96c0aba2c60d6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
