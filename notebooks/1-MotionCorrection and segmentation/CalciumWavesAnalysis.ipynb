{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Segmentation and motion correction of in-vivo recordings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before running this notebook, generate an ROI called `correctionReference.npy` using [Robopy](https://github.com/fedeceri85/robopy2). This ROI should be placed in the same folder as the recording and contain a trace where the \"jumps\" are visible as decreases (ideally) or increases in fluorescence.\n",
    "\n",
    "This notebook contains the preprocessing steps and segmentation routines for in vivo recordings. Some utilities used in this notebook are:\n",
    "\n",
    "- `thorlabsFile` from `movieTools`: This class contains the actual recording data, functions to smooth and filter it, and display it in a Napari viewer. The `thorlabsFile` object is passed to other routines to extract data. When loading files, they are automatically smoothed with a 3D Gaussian kernel (2x2x2).\n",
    "- `jumpFramesFinder` from `visualisationTools`: This displays an ipywidget interface with tools to remove out-of-focus frames. This interface displays the `correctionReference` generated using Robopy (first step of the analysis).\n",
    "- `jupyterpy` from `movieTools`: This interface connects to the currently displayed `thorlabsFile` and contains buttons for the segmentation of hair cells, calcium waves, and fibers.\n",
    "- `maskMatching` from `movieTools`: This is an ipywidgets/Napari interface that combines repetitive recordings of the same field of view to generate a `matchingMask`, i.e., a label image where hair cell ROIs are color-coded with the same color.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pylab\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import os\n",
    "from scipy.signal import savgol_filter\n",
    "from scipy.signal import find_peaks\n",
    "from scipy.signal import argrelextrema,argrelmin,argrelmax\n",
    "import sys\n",
    "sys.path.append('../../src')\n",
    "\n",
    "import seaborn as sns\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact, Dropdown\n",
    "import visualisationTools as vu\n",
    "import traceUtilities as tu\n",
    "import ast \n",
    "from movieTools import thorlabsFile\n",
    "\n",
    "parameterFolder = '../../parameters/'\n",
    "fileHeader = 'Pax2_Calciumwaves'\n",
    "\n",
    "inputFilename = os.path.join('../../',fileHeader)+'.xlsx'\n",
    "corrFilename = os.path.join(parameterFolder,fileHeader)+ '_with_corr_params.csv'\n",
    "jumpFrameFilename = os.path.join(parameterFolder,fileHeader+'jump_frames.csv')\n",
    "jumpFrameMaxFilename = os.path.join(parameterFolder,fileHeader+'jumpMax_frames.csv')\n",
    "correctionReferenceTraceFile = os.path.join(parameterFolder,fileHeader+'correctionReference.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "alldata = pd.read_excel(inputFilename)\n",
    "alldata = alldata[alldata['discard']!=1]\n",
    "\n",
    "try:\n",
    "    corr_param = pd.read_csv(corrFilename)\n",
    "except:\n",
    "    print('No jump correction parameter file')\n",
    "\n",
    "try:\n",
    "    allminima = pd.read_csv(jumpFrameFilename)\n",
    "except FileNotFoundError:\n",
    "    allminima = pd.DataFrame()\n",
    "\n",
    "try:\n",
    "    allmaxima = pd.read_csv(jumpFrameMaxFilename)\n",
    "except FileNotFoundError:\n",
    "    allmaxima = pd.DataFrame()\n",
    "\n",
    "\n",
    "try:\n",
    "    correctionReferenceTrace = pd.read_csv(correctionReferenceTraceFile)\n",
    "except FileNotFoundError:\n",
    "    correctionReferenceTrace = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for el in alldata['Folder'].unique():\n",
    "    try:\n",
    "        alldata.loc[alldata['Folder']==el,'Window left'] = corr_param.loc[corr_param['Folder']==el,'Window left'].values[0]\n",
    "        alldata.loc[alldata['Folder']==el,'Window right'] = corr_param.loc[corr_param['Folder']==el,'Window right'].values[0]\n",
    "        alldata.loc[alldata['Folder']==el,'Minima order'] = corr_param.loc[corr_param['Folder']==el,'Minima order'].values[0]\n",
    "\n",
    "\n",
    "\n",
    "    except (IndexError, NameError, KeyError):\n",
    "        print('Could not find correction parameters for '+ el+' Setting to default.')\n",
    "        alldata.loc[alldata['Folder']==el,'Window left'] = 0\n",
    "        alldata.loc[alldata['Folder']==el,'Window right'] = 0\n",
    "        alldata.loc[alldata['Folder']==el,'Minima order'] = 50 \n",
    "\n",
    "\n",
    "\n",
    "    try:\n",
    "        alldata.loc[alldata['Folder']==el,'Window Max left'] = corr_param.loc[corr_param['Folder']==el,'Window Max left'].values[0]\n",
    "        alldata.loc[alldata['Folder']==el,'Window Max right'] = corr_param.loc[corr_param['Folder']==el,'Window Max right'].values[0]\n",
    "        alldata.loc[alldata['Folder']==el,'Maxima order'] = corr_param.loc[corr_param['Folder']==el,'Maxima order'].values[0]\n",
    "    except (IndexError, NameError, KeyError):\n",
    "        alldata.loc[alldata['Folder']==el,'Window Max left'] = 0\n",
    "        alldata.loc[alldata['Folder']==el,'Window Max right'] = 0\n",
    "        alldata.loc[alldata['Folder']==el,'Maxima order'] = 50\n",
    "\n",
    "alldata.loc[alldata['Window left'].isna(),'Window left'] = 0\n",
    "alldata.loc[alldata['Window right'].isna(),'Window right'] = 0\n",
    "alldata.loc[alldata['Minima order'].isna(),'Minima order'] = 50\n",
    "\n",
    "alldata.loc[alldata['Folder'].isna(),'Window Max left'] = 0\n",
    "alldata.loc[alldata['Folder'].isna(),'Window Max right'] = 0\n",
    "alldata.loc[alldata['Folder'].isna(),'Maxima order'] = 50\n",
    "\n",
    "alldata['Window left'] = alldata['Window left'].astype(int)\n",
    "alldata['Window right'] = alldata['Window right'].astype(int)\n",
    "alldata['Minima order'] = alldata['Minima order'].astype(int)\n",
    "\n",
    "alldata['Window Max left'] = alldata['Window Max left'].astype(int)\n",
    "alldata['Window Max right'] = alldata['Window Max right'].astype(int)\n",
    "alldata['Maxima order'] = alldata['Maxima order'].astype(int)\n",
    "\n",
    "\n",
    "alldata.loc[alldata['Minima order']==1,'Minima order'] = 50\n",
    "alldata.loc[alldata['Maxima order']==1,'Maxima order'] = 50\n",
    "\n",
    "try:\n",
    "    for el in alldata['Folder'].unique():\n",
    "        alldata.loc[alldata['Folder']==el,'ExtraCorrectionIntervals'] = corr_param.loc[corr_param['Folder']==el,'ExtraCorrectionIntervals'].values[0]\n",
    "except:\n",
    "    pass\n",
    "\n",
    "for j,el in alldata.iterrows():\n",
    "    try:\n",
    "        alldata.at[j,'ExtraCorrectionIntervals'] = ast.literal_eval(alldata.at[j,'ExtraCorrectionIntervals'] )\n",
    "    except (ValueError,KeyError):\n",
    "         alldata.at[j,'ExtraCorrectionIntervals'] = np.nan\n",
    "alldata['ExtraCorrectionIntervals'] =  alldata['ExtraCorrectionIntervals'].astype('object')\n",
    "\n",
    "\n",
    "try:\n",
    "    for el in alldata['Folder'].unique():\n",
    "        alldata.loc[alldata['Folder']==el,'TemplateIntervals'] = corr_param.loc[corr_param['Folder']==el,'TemplateIntervals'].values[0]\n",
    "except:\n",
    "    pass\n",
    "\n",
    "for j,el in alldata.iterrows():\n",
    "    try:\n",
    "        alldata.at[j,'TemplateIntervals'] = ast.literal_eval(alldata.at[j,'TemplateIntervals'] )\n",
    "    except (ValueError,KeyError):\n",
    "         alldata.at[j,'TemplateIntervals'] = np.nan\n",
    "\n",
    "alldata['TemplateIntervals'] =  alldata['TemplateIntervals'].astype('object')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "master = alldata.copy().reset_index()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1- Jump Correction. \n",
    "Remember to run the block after this every now and then to save the results in case this notebook crashes\n",
    "`jumpFramesFinder` is a comprehensive tool that combines interactive widgets, data processing, and visualization to assist users in identifying and correcting frame jumps in their datasets.\n",
    "The jumpFramesFinder function is designed to facilitate the identification and correction of frame jumps in a in vivo movie. This function utilizes a variety of widgets from the `ipywidgets` library to create an interactive user interface  that allows users to adjust parameters and visualize the effects of these adjustments in real-time.\n",
    "\n",
    "The user can utilise various sliders to control various parameters such as the trace number, window sizes for minima and maxima detection, and the order of minima and maxima. These sliders allow users to fine-tune the detection of frame jumps by adjusting the sensitivity and range of the detection algorithm.\n",
    "\n",
    "Users can perform specific actions, such as loading original or corrected movie, saving processed files, and creating templates for further analysis.\n",
    "\n",
    "The function also sets up a plot using `plotly.graph_objects` to visualize the original and corrected traces, as well as the detected jumps. This visualization helps users to see the impact of their adjustments and make decisions about the parameters they set.\n",
    "\n",
    "The corrected movie is visualised through the `thorlabsFile` object in a napari window.\n",
    "\n",
    "After this step, launch the `batchModtionCorrect.ipynb` notebook to motion correct the \"jumpCorrected.tif\" files, which will be saved as `jumpCorrected-mc.tif` files. After that, come back to this notebook, and run this block to load the motion corrected files, to be used for segmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tb = thorlabsFile()\n",
    "vu.jumpFramesFinder(master,allminima,allmaxima,correctionReferenceTrace,tb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save parameters block. Uncomment it and run it while doing the previous step to prevent data loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#master.to_csv(corrFilename)\n",
    "#allminima.to_csv(jumpFrameFilename)\n",
    "#allmaxima.to_csv(jumpFrameMaxFilename)\n",
    "#correctionReferenceTrace.to_csv(correctionReferenceTraceFile)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The next block displays the jupyterpy interface. This connects to the opened image in napari (the current thorlabsFile object, it should be loaded using the jumpFramesFinder above) and provides buttons with specific analysis steps for each type of experiment.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The jupyterPy function is designed to create an interactive user interface (UI) for analyzing image data within a Jupyter notebook. This function utilizes the ipywidgets library to create various interactive widgets, such as buttons, sliders, and text inputs, which are then organized into a tabbed layout for different analysis tasks.\n",
    "The istance of jupyterPy connects to the movie displayed in the existing napari windows (passed through the thorlabsFile object.)\n",
    "\n",
    "The software allows to perform specific segmentation analysis for IHCs, Calcium waves, and afferent terminals. \n",
    "For IHCs, the software uses cellpose to automatically identify the cell bodies. The segmentation is shown as a `label` layer (named 'Masks') in Napari. Users can modify the existing ROIs using Napari tools (e.g, splitting ROIs). The function also allows to plot and save the fluorescence profile of the individual ROIs. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instructions for Ca wave detection:\n",
    "\n",
    "\n",
    "1 - Start from the raw image. Select a background frmae where the fluorescence is quite low (now always at the beginning). Subtract it\n",
    "\n",
    "2 - Subtract an integer number (Subtract X Background) to reduce noise. Usually 20 is good.\n",
    "\n",
    "3 - If desired, bin 2 the image (a quarter of the frame size) to save time. \n",
    "\n",
    "4 - If desired, downsample time (one frame every 2)\n",
    "\n",
    "5 - Select spot and outline sigma for the voronoi.\n",
    "\n",
    "6 - execute the voronoi.\n",
    "\n",
    "7 - inspect the labels, delete single supoorting cell events and merge/split labels using Plugins->Napari segment blobs and things...->manually merge/split labels. \n",
    "\n",
    "8 - If labels are deleted or merged, rearrange the labels so they are numbered sequentially.\n",
    "\n",
    "9 - If the recording has been scaled at the beginning, scale back the labels to original size before saving!\n",
    "\n",
    "10 - Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from movieTools import jupyterPy\n",
    "JP = jupyterPy(tb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate kymographs for all recordings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cupy as cp\n",
    "from cupyx.scipy import ndimage\n",
    "import tifffile \n",
    "def calculateKymo(data, hcline):\n",
    "    \"\"\"\n",
    "    Calculate kymographs from the given data along a specified line.\n",
    "    \n",
    "    Parameters:\n",
    "    data (numpy.ndarray): 3D array of shape (frames, height, width) containing the image data.\n",
    "    hcline (numpy.ndarray): 2D array of shape (n_points, 2) containing the coordinates of a line drawn on the IHCs.\n",
    "    \n",
    "    Returns:\n",
    "    numpy.ndarray: 2D array containing the kymographs.\n",
    "    \"\"\"\n",
    "    MAXCHUNKSIZE = 10000  # Maximum chunk size for processing\n",
    "    Nchunks = data.shape[0] // MAXCHUNKSIZE  # Number of chunks\n",
    "\n",
    "    kymos = []\n",
    "    # Calculate the coordinates along the line\n",
    "    x = []\n",
    "    y = []\n",
    "    for i in range(hcline.shape[0] - 1):\n",
    "        x0 = np.arange(hcline[i, 0], hcline[i + 1, 0])\n",
    "        x.append(x0)\n",
    "        y.append(np.linspace(hcline[i, 1], hcline[i + 1, 1], x0.size))\n",
    "    x = np.hstack(x)\n",
    "    y = np.hstack(y)\n",
    "    cphcline = cp.array(np.vstack([y, x]))  # Convert to CuPy array\n",
    "\n",
    "    for chunk in range(Nchunks):\n",
    "        cpdata = cp.array(data[chunk * MAXCHUNKSIZE:(chunk + 1) * MAXCHUNKSIZE, :, :])  # Load chunk into GPU\n",
    "        \n",
    "        # Loop through the frames\n",
    "        kymo = []\n",
    "        for i in range(cpdata.shape[0]):\n",
    "            img1 = ndimage.map_coordinates(cpdata[i, :, :], cphcline)\n",
    "            # Average over neighboring pixels\n",
    "            for j in [-2, -1, 1, 2]:\n",
    "                img1 += ndimage.map_coordinates(cpdata[i, :, :], cphcline + cp.array([[j], [0]]))\n",
    "            kymo.append(img1 / 5.0)\n",
    "        kymos.append(np.vstack(kymo).get())  # Move result back to CPU\n",
    "\n",
    "        del cpdata\n",
    "        cp.get_default_memory_pool().free_all_blocks()  # Free GPU memory\n",
    "    \n",
    "    # Process remaining frames if any\n",
    "    if data.shape[0] % MAXCHUNKSIZE != 0:\n",
    "        cpdata = cp.array(data[Nchunks * MAXCHUNKSIZE:, :, :])\n",
    "        \n",
    "        # Loop through the frames\n",
    "        kymo = []\n",
    "        for i in range(cpdata.shape[0]):\n",
    "            img1 = ndimage.map_coordinates(cpdata[i, :, :], cphcline)\n",
    "            # Average over neighboring pixels\n",
    "            for j in [-2, -1, 1, 2]:\n",
    "                img1 += ndimage.map_coordinates(cpdata[i, :, :], cphcline + cp.array([[j], [0]]))\n",
    "            kymo.append(img1 / 5.0)\n",
    "        kymos.append(np.vstack(kymo).get())  # Move result back to CPU\n",
    "\n",
    "        del cpdata\n",
    "        cp.get_default_memory_pool().free_all_blocks()  # Free GPU memory\n",
    "\n",
    "    kymos = np.vstack(kymos)  # Combine all chunks\n",
    "    return kymos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate kymographs for all the data\n",
    "drive = 'D'\n",
    "movementum = 20#um how much to shift the line in y to intersect the bulk of the ger instead of the ihcs\n",
    "for i,el in master.iterrows():\n",
    "    #if i ==44:\n",
    "        folder = drive+el['Folder'][1:]\n",
    "        print(folder,i)\n",
    "        hcline = pd.read_csv(os.path.join(folder,'hcs.csv'))\n",
    "        umpx = el['um/pixel']\n",
    "        movementY = movementum / umpx\n",
    "\n",
    "        hcline_shifted = hcline - [0,movementY] \n",
    "\n",
    "        data = tifffile.imread(os.path.join(folder,'processedMovies','1-jumpCorrected-mc.tif'))\n",
    "\n",
    "        kymoHC = calculateKymo(data,hcline.values)\n",
    "        kymoSCs = calculateKymo(data,hcline_shifted.values)\n",
    "        \n",
    "        img = np.stack((kymoHC,kymoSCs,zeros(kymoHC.shape)),axis=0)\n",
    "        tifffile.imwrite(os.path.join(folder,'processedMovies','kymoHCs.tif'),img,metadata = {'axes':'YXC','PhysicalSizeX': umpx, 'PhysicalSizeXUnit': 'Âµm',\n",
    "                                                                'PhysicalSizeY': 1.0/el['fps'],'PhysicalSizeYUnit': 's'},photometric='rgb')\n",
    "\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate extension of hair cell vs scs calcium waves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "import shapely"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masterIHCvsSC = pd.DataFrame()\n",
    "\n",
    "# Iterate through each row in the master dataframe\n",
    "for i, el in master.iterrows():\n",
    "    # Check if the kymoShapes.csv file exists in the processedMovies folder\n",
    "    if os.path.exists(os.path.join(el['Folder'], 'processedMovies', 'kymoShapes.csv')):\n",
    "        df = pd.read_csv(os.path.join(el['Folder'], 'processedMovies', 'kymoShapes.csv'))\n",
    "        \n",
    "        # Ensure there is an even number of waves (last index must be odd)\n",
    "        if df['index'].unique().max() % 2 != 1:\n",
    "            print('Error, waves not matched')\n",
    "\n",
    "        # Iterate through each unique index in the dataframe\n",
    "        for index in df['index'].unique():\n",
    "            this_masterIHCSSCS = pd.Series(dtype=float64)\n",
    "            this_df = df[df['index'] == index]\n",
    "            \n",
    "            # Determine the event type based on the shape type\n",
    "            if this_df['shape-type'].iloc[0] == 'rectangle':\n",
    "                this_masterIHCSSCS['Event type'] = 'IHCs'\n",
    "            elif this_df['shape-type'].iloc[0] == 'ellipse':\n",
    "                this_masterIHCSSCS['Event type'] = 'GER'\n",
    "            \n",
    "            # Calculate event extension in micrometers\n",
    "            this_masterIHCSSCS['Event extension um'] = (this_df['axis-1'].max() - this_df['axis-1'].min()) * el['um/pixel']\n",
    "            this_masterIHCSSCS['Time (frame)'] = this_df['axis-0'].min()\n",
    "            this_masterIHCSSCS['Time (s)'] = this_df['axis-0'].min() * el['fps']\n",
    "            this_masterIHCSSCS['Folder'] = el['Folder']\n",
    "            this_masterIHCSSCS['Mouse ID'] = el['Mouse ID']\n",
    "            this_masterIHCSSCS['Age'] = el['Age']\n",
    "            \n",
    "            # Create a polygon from the axis-1 and axis-0 coordinates\n",
    "            this_masterIHCSSCS['Polygon'] = shapely.geometry.Polygon(list(zip(this_df['axis-1'], this_df['axis-0'])))\n",
    "\n",
    "            # Append the series to the masterIHCvsSC dataframe\n",
    "            masterIHCvsSC = masterIHCvsSC.append(this_masterIHCSSCS, ignore_index=True)\n",
    "\n",
    "# Parse the dataframe two by two, check that one event is IHCs and one is GER, and check that the two events intersect in the kymograph\n",
    "for j in range(masterIHCvsSC.shape[0] // 2):\n",
    "    pol1 = masterIHCvsSC.iloc[j * 2]['Polygon']\n",
    "    pol2 = masterIHCvsSC.iloc[j * 2 + 1]['Polygon']\n",
    "    type1 = masterIHCvsSC.iloc[j * 2]['Event type']\n",
    "    type2 = masterIHCvsSC.iloc[j * 2 + 1]['Event type']\n",
    "\n",
    "    index = masterIHCvsSC.index[j * 2]\n",
    "    index2 = masterIHCvsSC.index[j * 2 + 1]\n",
    "    \n",
    "    # Check if the polygons intersect\n",
    "    if not pol1.intersects(pol2):\n",
    "        print('ERROR ! not intersecting consecutive shapes')\n",
    "    # Check if the two consecutive events are of the same type\n",
    "    elif type1 == type2:\n",
    "        print('ERROR! two events of the same type consecutive')\n",
    "    else:\n",
    "        # Assign a unique event number to the intersecting events\n",
    "        masterIHCvsSC.loc[index, 'Unique event number'] = j\n",
    "        masterIHCvsSC.loc[index2, 'Unique event number'] = j\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masterIHCvsSC.to_csv('..\\\\..\\\\Data for figures\\\\CalciumWaves\\\\masterIHCvsSC_kymographquantification.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# N numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master.groupby(['Age','Mouse ID']).count()['Folder']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "df12da531320a3294192bd41b194b56443186719b27973241873f2b85ffbcaf8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
